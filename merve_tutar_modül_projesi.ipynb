{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/img/huawei_logo.png\"  style=\"width:160px;\"/>\n",
    "<div style=\"background-color: white; padding: 10px; border-bottom: 6px solid #C2172D;\">\n",
    "    <h2 style=\"color: black\" id=\"introduction\">Batch Data Processing with Apache Spark</h2>\n",
    "    <p>Tolgahan Cepel - Mert Akat</p>\n",
    "    <p></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Contents](#contents)\n",
    "1. [Introduction](#introduction)\n",
    "2. [Importing the libraries](#library)\n",
    "3. [Reading the data](#read_data)\n",
    "4. [SparkSQL API Practices](#spark_sql_practices)\n",
    "   * [Selecting columns](#selecting_columns)\n",
    "   * [Data manipulation](#data_manipulation)\n",
    "   * [Filtering rows](#filtering_rows)\n",
    "   * [Aggregating data](#aggregating_data)\n",
    "   * [Joining](#joining)\n",
    "5. [Case Studies](#assignments)\n",
    "   * [Assignment 1: Jacket sales per region](#assignment_1)\n",
    "   * [Assignment 2: Maximum turnover of the retailer regions](#assignment_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; padding: 10px; border-bottom: 4px solid #C2172D;\">\n",
    "    <a id=\"introduction\">\n",
    "        <h3 style=\"color: #C2172D\">1. Introduction</h3>\n",
    "    </a>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/img/data_model.svg\"  style=\"width:1000px; padding: 20px\"/>\n",
    "\n",
    "#### SQL Tables Description\n",
    "- **FactSale:** Sales transactions fact table\n",
    "- **FactPurchase:** Purchases fact table\n",
    "- **DimRetailer:** Retailer details dimension table\n",
    "- **DimCustomer:** Customer details dimension table\n",
    "- **DimProduct:** Product details dimension table\n",
    "- **DimRegion:** Region details dimension table\n",
    "- **DimDate:** Date dimension table\n",
    "- **DimSupplier:** Supplier details dimension table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; padding: 10px; border-bottom: 4px solid #C2172D;\">\n",
    "    <a id=\"library\">\n",
    "        <h3 style=\"color: #C2172D\">2. Importing the libraries</h3>\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; padding: 10px; border-bottom: 4px solid #C2172D;\">\n",
    "    <a id=\"read_data\">\n",
    "        <h3 style=\"color: #C2172D\">3. Reading the data</h3>\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new SparkSession instance\n",
    "spark = SparkSession.builder.appName(\"PySparkExample\").getOrCreate()\n",
    "\n",
    "# Reading parquet data and assigning to DataFrame variables\n",
    "df_pur = spark.read.parquet(\"data/purchase\")\n",
    "df_sal = spark.read.parquet(\"data/sale\")\n",
    "df_cus = spark.read.parquet(\"data/customer\")\n",
    "df_ret = spark.read.parquet(\"data/retailer\")\n",
    "df_pro = spark.read.parquet(\"data/product\")\n",
    "df_sup = spark.read.parquet(\"data/supplier\")\n",
    "df_reg = spark.read.parquet(\"data/region\")\n",
    "df_date = spark.read.parquet(\"data/date\")\n",
    "\n",
    "# Creating temporary view tables for Spark SQL queries\n",
    "df_cus.createOrReplaceTempView(\"DimCustomer\")\n",
    "df_pur.createOrReplaceTempView(\"FactPurchase\")\n",
    "df_sal.createOrReplaceTempView(\"FactSale\")\n",
    "df_ret.createOrReplaceTempView(\"DimRetailer\")\n",
    "df_pro.createOrReplaceTempView(\"DimProduct\")\n",
    "df_sup.createOrReplaceTempView(\"DimSupplier\")\n",
    "df_reg.createOrReplaceTempView(\"DimRegion\")\n",
    "df_date.createOrReplaceTempView(\"DimDate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; padding: 10px; border-bottom: 4px solid #C2172D;\">\n",
    "    <a id=\"spark_sql_practices\">\n",
    "        <h3 style=\"color: #C2172D\">4. Spark SQL Practices</h3>\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<a id=\"selecting_columns\">Selecting columns</a>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+--------+----------+\n",
      "|customer_id|   name| surname|birth_date|\n",
      "+-----------+-------+--------+----------+\n",
      "|          1| Jazmin|  Burril|1958-09-22|\n",
      "|          2| Dalila|   Faers|2000-11-08|\n",
      "|          3|Wayland|Walework|1976-03-08|\n",
      "|          4|Amberly|  Haquin|1948-10-08|\n",
      "|          5|Garrett|   Frear|1957-09-25|\n",
      "+-----------+-------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT customer_id, name, surname, birth_date FROM DimCustomer LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+--------+----------+\n",
      "|customer_id|   name| surname|birth_date|\n",
      "+-----------+-------+--------+----------+\n",
      "|          1| Jazmin|  Burril|1958-09-22|\n",
      "|          2| Dalila|   Faers|2000-11-08|\n",
      "|          3|Wayland|Walework|1976-03-08|\n",
      "|          4|Amberly|  Haquin|1948-10-08|\n",
      "|          5|Garrett|   Frear|1957-09-25|\n",
      "+-----------+-------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cus.select(\"customer_id\", \"name\", \"surname\", \"birth_date\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<a id=\"data_manipulation\">Data manipulation: </a>** Calculating the ages from date of birth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+--------+---+\n",
      "|customer_id|   name| surname|age|\n",
      "+-----------+-------+--------+---+\n",
      "|          1| Jazmin|  Burril| 66|\n",
      "|          2| Dalila|   Faers| 24|\n",
      "|          3|Wayland|Walework| 48|\n",
      "|          4|Amberly|  Haquin| 76|\n",
      "|          5|Garrett|   Frear| 67|\n",
      "+-----------+-------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    customer_id\n",
    "    ,name\n",
    "    ,surname\n",
    "    ,YEAR(CURRENT_DATE()) - YEAR(birth_date) AS age\n",
    "FROM DimCustomer\n",
    "LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+--------+---+\n",
      "|customer_id|   name| surname|age|\n",
      "+-----------+-------+--------+---+\n",
      "|          1| Jazmin|  Burril| 66|\n",
      "|          2| Dalila|   Faers| 24|\n",
      "|          3|Wayland|Walework| 48|\n",
      "|          4|Amberly|  Haquin| 76|\n",
      "|          5|Garrett|   Frear| 67|\n",
      "+-----------+-------+--------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_cus.withColumn(\"age\", F.year(F.current_date()) - F.year(\"birth_date\"))\n",
    "    .select(\"customer_id\", \"name\", \"surname\", \"age\")\n",
    "    .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<a id=\"filtering_rows\">Filtering rows</a>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---+\n",
      "|   name| surname|age|\n",
      "+-------+--------+---+\n",
      "| Jazmin|  Burril| 66|\n",
      "|Wayland|Walework| 48|\n",
      "|Amberly|  Haquin| 76|\n",
      "|Garrett|   Frear| 67|\n",
      "|  Horst|   Isted| 49|\n",
      "+-------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    name\n",
    "    ,surname\n",
    "    ,age\n",
    "FROM\n",
    "(\n",
    "    SELECT\n",
    "        customer_id\n",
    "        ,name\n",
    "        ,surname\n",
    "        ,YEAR(CURRENT_DATE()) - YEAR(birth_date) AS age\n",
    "    FROM DimCustomer\n",
    ")\n",
    "WHERE age >= 30\n",
    "LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---+\n",
      "|   name| surname|age|\n",
      "+-------+--------+---+\n",
      "| Jazmin|  Burril| 66|\n",
      "|Wayland|Walework| 48|\n",
      "|Amberly|  Haquin| 76|\n",
      "|Garrett|   Frear| 67|\n",
      "|  Horst|   Isted| 49|\n",
      "+-------+--------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_cus.withColumn(\"age\", F.year(F.current_date()) - F.year(\"birth_date\"))\n",
    "    .select(\"name\", \"surname\", \"age\")\n",
    "    .filter(F.col(\"age\") >= 30)\n",
    "    .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<a id=\"aggregating_data\">Aggregating data</a>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+------------+\n",
      "|order_id|total_quantity|total_amount|\n",
      "+--------+--------------+------------+\n",
      "|    3647|            13|         521|\n",
      "|    2574|            13|         488|\n",
      "|    3515|            13|         402|\n",
      "|     101|            12|         359|\n",
      "|     440|            12|         426|\n",
      "|    3763|            12|         323|\n",
      "|    1585|            12|         488|\n",
      "|    3289|            12|         327|\n",
      "|    1382|            11|         452|\n",
      "|    1752|            11|         298|\n",
      "+--------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    order_id\n",
    "    ,SUM(quantity) AS total_quantity\n",
    "    ,SUM(total_amt) AS total_amount\n",
    "FROM FactSale\n",
    "GROUP BY order_id\n",
    "ORDER BY total_quantity DESC\n",
    "LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+------------+\n",
      "|order_id|total_quantity|total_amount|\n",
      "+--------+--------------+------------+\n",
      "|    3647|            13|         521|\n",
      "|    2574|            13|         488|\n",
      "|    3515|            13|         402|\n",
      "|     101|            12|         359|\n",
      "|     440|            12|         426|\n",
      "|    3763|            12|         323|\n",
      "|    1585|            12|         488|\n",
      "|    3289|            12|         327|\n",
      "|    2337|            11|         357|\n",
      "|    3743|            11|         359|\n",
      "+--------+--------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_sal.groupBy(\"order_id\").agg(\n",
    "        F.sum(\"quantity\").alias(\"total_quantity\"),\n",
    "        F.sum(\"total_amt\").alias(\"total_amount\")\n",
    "    ).orderBy(\"total_quantity\", ascending=False)\n",
    "    .show(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<a id=\"joining\">Joining</a>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|      region_name|               age|\n",
      "+-----------------+------------------+\n",
      "|          Akdeniz| 50.81521739130435|\n",
      "|     Dogu Anadolu| 50.13095238095238|\n",
      "|Guneydogu Anadolu| 48.58119658119658|\n",
      "|          Marmara|48.189542483660134|\n",
      "|       Ic Anadolu| 48.07772020725388|\n",
      "|        Karadeniz| 47.75121951219512|\n",
      "|              Ege|46.888888888888886|\n",
      "+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    region_name\n",
    "    ,AVG(YEAR(CURRENT_DATE()) - YEAR(birth_date)) AS age\n",
    "FROM DimCustomer cus\n",
    "INNER JOIN DimRegion reg\n",
    "ON cus.city_id = reg.city_id\n",
    "GROUP BY region_name\n",
    "ORDER BY age DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|      region_name|               age|\n",
      "+-----------------+------------------+\n",
      "|          Akdeniz| 50.81521739130435|\n",
      "|     Dogu Anadolu| 50.13095238095238|\n",
      "|Guneydogu Anadolu| 48.58119658119658|\n",
      "|          Marmara|48.189542483660134|\n",
      "|       Ic Anadolu| 48.07772020725388|\n",
      "|        Karadeniz| 47.75121951219512|\n",
      "|              Ege|46.888888888888886|\n",
      "+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_cus\n",
    "    .join(df_reg, df_cus.city_id == df_reg.city_id)\n",
    "    .groupBy(\"region_name\").agg(\n",
    "        F.avg(F.year(F.current_date()) - F.year(\"birth_date\")).alias(\"age\")\n",
    "    )\n",
    "    .orderBy(\"age\", ascending=False)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; padding: 10px; border-bottom: 4px solid #C2172D;\">\n",
    "    <a id=\"case_studies\">\n",
    "        <h3 style=\"color: #C2172D\">5. Case Studies</h3>\n",
    "    </a>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; padding: 10px;\">\n",
    "    <a id=\"assignment_1\">\n",
    "        <h4 style=\"color: #0D9276\">Assignment 1: Jacket sales per region</h3>\n",
    "    </a>\n",
    "</div>\n",
    "<br>\n",
    "<h4>\n",
    "    Write SparkSQL and Python API scripts that results: Region-based total quantity and amount of jacket sales between June and August 2023.\n",
    "</h4>\n",
    "<p>The expected out is as follows: </p>\n",
    "\n",
    "| region_name       | product_type | total_quantity | total_amount |   |\n",
    "|-------------------|--------------|----------------|--------------|---|\n",
    "| Marmara           | Jacket       | 213            | 8358         |   |\n",
    "| Dogu Anadolu      | Jacket       | 284            | 11547        |   |\n",
    "| Guneydogu Anadolu | Jacket       | 176            | 6981         |   |\n",
    "| Ic Anadolu        | Jacket       | 260            | 10496        |   |\n",
    "| Akdeniz           | Jacket       | 162            | 6637         |   |\n",
    "| Karadeniz         | Jacket       | 310            | 12582        |   |\n",
    "| Ege               | Jacket       | 101            | 3953      \n",
    "\n",
    "\n",
    "### External links\n",
    "- https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.join.html\n",
    "- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.filter.html\n",
    "- https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.groupBy.html   |   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+--------------+------------+\n",
      "|      region_name|product_type|total_quantity|total_amount|\n",
      "+-----------------+------------+--------------+------------+\n",
      "|          Marmara|      Jacket|           213|        8358|\n",
      "|     Dogu Anadolu|      Jacket|           284|       11547|\n",
      "|Guneydogu Anadolu|      Jacket|           176|        6981|\n",
      "|       Ic Anadolu|      Jacket|           260|       10496|\n",
      "|          Akdeniz|      Jacket|           162|        6637|\n",
      "|        Karadeniz|      Jacket|           310|       12582|\n",
      "|              Ege|      Jacket|           101|        3953|\n",
      "+-----------------+------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your Spark SQL Solution:\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import month, year, sum, lit\n",
    "# Filtering jacket sales between June and August 2023\n",
    "jacket_sales = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        reg.region_name,\n",
    "        prod.product_type,\n",
    "        SUM(sal.quantity) AS total_quantity,\n",
    "        SUM(sal.total_amt) AS total_amount\n",
    "    FROM\n",
    "        FactSale sal\n",
    "    INNER JOIN\n",
    "        DimProduct prod ON sal.product_id = prod.product_id\n",
    "    INNER JOIN\n",
    "        DimCustomer cus ON sal.customer_id = cus.customer_id\n",
    "    INNER JOIN\n",
    "        DimRegion reg ON cus.city_id = reg.city_id\n",
    "    INNER JOIN\n",
    "        DimDate dat ON sal.date = dat.date\n",
    "    WHERE\n",
    "        prod.product_type = 'Jacket'\n",
    "        AND dat.month BETWEEN 6 AND 8\n",
    "        AND dat.year = 2023\n",
    "    GROUP BY\n",
    "        reg.region_name, prod.product_type\n",
    "    \n",
    "\"\"\")\n",
    "\n",
    "# Displaying the result\n",
    "jacket_sales.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+--------------+------------+\n",
      "|      region_name|product_type|total_quantity|total_amount|\n",
      "+-----------------+------------+--------------+------------+\n",
      "|          Marmara|      Jacket|           213|        8358|\n",
      "|     Dogu Anadolu|      Jacket|           284|       11547|\n",
      "|Guneydogu Anadolu|      Jacket|           176|        6981|\n",
      "|       Ic Anadolu|      Jacket|           260|       10496|\n",
      "|          Akdeniz|      Jacket|           162|        6637|\n",
      "|        Karadeniz|      Jacket|           310|       12582|\n",
      "|              Ege|      Jacket|           101|        3953|\n",
      "+-----------------+------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your PySpark Solution:\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Ceket satışlarını filtreleme ve bölgeye göre gruplama\n",
    "jacket_sales_per_region = df_sal.join(df_pro, df_sal.product_id == df_pro.product_id) \\\n",
    "    .join(df_cus, df_sal.customer_id == df_cus.customer_id) \\\n",
    "    .join(df_reg, df_cus.city_id == df_reg.city_id) \\\n",
    "    .join(df_date, df_sal.date == df_date.date) \\\n",
    "    .filter((df_pro.product_type == 'Jacket') &\n",
    "            (F.month(df_date.date).between(6, 8)) &\n",
    "            (df_date.year == 2023)) \\\n",
    "    .groupBy(\"region_name\", \"product_type\") \\\n",
    "    .agg(F.sum(\"quantity\").alias(\"total_quantity\"), \n",
    "         F.sum(\"total_amt\").alias(\"total_amount\"))\n",
    "\n",
    "# Sonucu gösterme\n",
    "jacket_sales_per_region.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; padding: 10px;\">\n",
    "    <a id=\"assignment_2\">\n",
    "        <h4 style=\"color: #0D9276\">Assignment 2: Maximum turnover of the retailer regions</h3>\n",
    "    </a>\n",
    "</div>\n",
    "<br>\n",
    "<h4>\n",
    "    Find the maximum turnover region of each retailer, and obtain total amount for each retailer and region.\n",
    "</h4>\n",
    "<p>The expected out is as follows: </p>\n",
    "\n",
    "| retailer_id | retailer_name | region_name | total_amount |\n",
    "|-------------|---------------|-------------|--------------|\n",
    "| 1           | Hepsiburada   | Karadeniz   | 42642        |\n",
    "| 2           | Trendyol      | Ic Anadolu  | 71689        |\n",
    "| 3           | n11           | Ic Anadolu  | 11995        |\n",
    "| 4           | Gittigidiyor  | Karadeniz   | 16081        |\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+------------+\n",
      "|retailer_id|retailer_name|region_name|total_amount|\n",
      "+-----------+-------------+-----------+------------+\n",
      "|          1|  Hepsiburada|  Karadeniz|       42642|\n",
      "|          2|     Trendyol| Ic Anadolu|       71689|\n",
      "|          3|          n11| Ic Anadolu|       11995|\n",
      "|          4| Gittigidiyor|  Karadeniz|       16081|\n",
      "+-----------+-------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your Spark SQL Solution:\n",
    "\n",
    "\n",
    "max_turnover_per_retailer = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        retailer_id,\n",
    "        retailer_name,\n",
    "        region_name,\n",
    "        total_amount\n",
    "    FROM (\n",
    "        SELECT \n",
    "            retailer_id,\n",
    "            retailer_name,\n",
    "            region_name,\n",
    "            total_amount,\n",
    "            ROW_NUMBER() OVER(PARTITION BY retailer_id ORDER BY total_amount DESC) AS rn\n",
    "        FROM (\n",
    "            SELECT \n",
    "                ret.retailer_id, \n",
    "                ret.retailer_name, \n",
    "                reg.region_name, \n",
    "                SUM(sal.total_amt) AS total_amount\n",
    "            FROM \n",
    "                FactSale sal\n",
    "            JOIN \n",
    "                DimRetailer ret ON sal.retailer_id = ret.retailer_id\n",
    "            JOIN \n",
    "                DimCustomer cus ON sal.customer_id = cus.customer_id\n",
    "            JOIN \n",
    "                DimRegion reg ON cus.city_id = reg.city_id\n",
    "            GROUP BY \n",
    "                ret.retailer_id, ret.retailer_name, reg.region_name\n",
    "        ) turnover_per_region\n",
    "    ) ranked_turnover\n",
    "    WHERE rn = 1\n",
    "\"\"\")\n",
    "\n",
    "# Displaying the result\n",
    "max_turnover_per_retailer.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+------------+\n",
      "|retailer_id|retailer_name|region_name|total_amount|\n",
      "+-----------+-------------+-----------+------------+\n",
      "|          1|  Hepsiburada|  Karadeniz|       42642|\n",
      "|          2|     Trendyol| Ic Anadolu|       71689|\n",
      "|          3|          n11| Ic Anadolu|       11995|\n",
      "|          4| Gittigidiyor|  Karadeniz|       16081|\n",
      "+-----------+-------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your PySpark Solution:\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Joining necessary tables with DataFrame aliases\n",
    "joined_df = df_sal.alias(\"sal\").join(df_ret.alias(\"ret\"), F.col(\"sal.retailer_id\") == F.col(\"ret.retailer_id\")) \\\n",
    "                  .join(df_cus.alias(\"cus\"), F.col(\"sal.customer_id\") == F.col(\"cus.customer_id\")) \\\n",
    "                  .join(df_reg.alias(\"reg\"), F.col(\"cus.city_id\") == F.col(\"reg.city_id\"))\n",
    "\n",
    "# Grouping data by retailer_id, region_name, and aggregating total amount\n",
    "grouped_df = joined_df.groupBy(\"ret.retailer_id\", \"ret.retailer_name\", \"reg.region_name\") \\\n",
    "                      .agg(F.sum(\"total_amt\").alias(\"total_amount\"))\n",
    "\n",
    "# Defining a window partitioned by retailer_id and ordered by total_amount descending\n",
    "window_spec = Window.partitionBy(\"ret.retailer_id\").orderBy(F.desc(\"total_amount\"))\n",
    "\n",
    "# Adding a rank column to find the maximum turnover region for each retailer\n",
    "result_df = grouped_df.withColumn(\"rank\", F.rank().over(window_spec)) \\\n",
    "                      .filter(F.col(\"rank\") == 1) \\\n",
    "                      .drop(\"rank\")\n",
    "\n",
    "# Renaming columns and ordering the result\n",
    "result_df = result_df.select(\"ret.retailer_id\", \"ret.retailer_name\", \"reg.region_name\", \"total_amount\") \\\n",
    "                     .orderBy(\"ret.retailer_id\")\n",
    "\n",
    "# Showing the result\n",
    "result_df.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
